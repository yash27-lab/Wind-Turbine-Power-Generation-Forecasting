# -*- coding: utf-8 -*-
"""Wind Turbine Power Generation Forecastingipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18N0pIOmK9Q4F26eOUnke77XfYBzuCtdd
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning Libraries
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.ensemble import RandomForestRegressor

# For time series decomposition
import statsmodels.api as sm

# Suppress warnings for cleaner output
import warnings
warnings.filterwarnings('ignore')

# ------------------------------
# 1. Data Loading and Preparation
# ------------------------------

# Load training data
train_df = pd.read_excel('Train.xlsx')

# Load test data
test_df = pd.read_csv('Test.csv')

# Display first few rows of the training data
print("First few rows of training data:")
print(train_df.head())

# Ensure 'Timestamp' column is correctly read
if 'Unnamed: 0' in train_df.columns:
    train_df.rename(columns={'Unnamed: 0': 'Timestamp'}, inplace=True)
if 'Unnamed: 0' in test_df.columns:
    test_df.rename(columns={'Unnamed: 0': 'Timestamp'}, inplace=True)

# Convert 'Timestamp' to datetime
train_df['Timestamp'] = pd.to_datetime(train_df['Timestamp'])
test_df['Timestamp'] = pd.to_datetime(test_df['Timestamp'])

# Sort training data by 'Timestamp'
train_df.sort_values('Timestamp', inplace=True)
train_df.reset_index(drop=True, inplace=True)

# ------------------------------
# 2. Exploratory Data Analysis (EDA)
# ------------------------------

# Plot Power Output Over Time
plt.figure(figsize=(15,5))
plt.plot(train_df['Timestamp'], train_df['Power'], label='Power Output')
plt.xlabel('Time')
plt.ylabel('Power Output (kW)')
plt.title('Wind Turbine Power Output Over Time')
plt.legend()
plt.show()

# Correlation Matrix
plt.figure(figsize=(12,10))
sns.heatmap(train_df.corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Distribution of Features
train_df.hist(figsize=(15,10))
plt.show()

# Decompose Time Series
power_ts = train_df.set_index('Timestamp')['Power']
decomposition = sm.tsa.seasonal_decompose(power_ts, model='additive', period=24)
fig = decomposition.plot()
plt.show()

# ------------------------------
# 3. Data Preprocessing
# ------------------------------

# Handle missing values by dropping (assuming few missing values)
train_df.dropna(inplace=True)
train_df.reset_index(drop=True, inplace=True)

# ------------------------------
# 4. Feature Engineering
# ------------------------------

# Create time-based features
for df in [train_df, test_df]:
    df['Hour'] = df['Timestamp'].dt.hour
    df['Day'] = df['Timestamp'].dt.day
    df['Month'] = df['Timestamp'].dt.month
    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek
    df['IsWeekend'] = df['DayOfWeek'].isin([5,6]).astype(int)

# Create lag features in training data
lags = [1, 2, 3, 24]  # Lags in hours
for lag in lags:
    train_df[f'Power_lag_{lag}'] = train_df['Power'].shift(lag)

# Drop rows with NaN values resulting from lag features
train_df.dropna(inplace=True)
train_df.reset_index(drop=True, inplace=True)

# Create rolling statistics in training data
window_sizes = [3, 6, 12]  # Window sizes in hours
for window in window_sizes:
    train_df[f'RollingMean_{window}'] = train_df['Power'].rolling(window=window).mean()
    train_df[f'RollingStd_{window}'] = train_df['Power'].rolling(window=window).std()

# Drop NaN values from rolling statistics
train_df.dropna(inplace=True)
train_df.reset_index(drop=True, inplace=True)

# Create interaction feature
if 'WS_10m' in train_df.columns:
    train_df['WindSpeed_Power'] = train_df['WS_10m'] * train_df['Power']
else:
    print("'WS_10m' not found in train_df.")

# ------------------------------
# 5. Prepare Training Data
# ------------------------------

# Define target variable
target = 'Power'

# Exclude 'Timestamp' and target from features
features = train_df.columns.drop(['Timestamp', target])

# Ensure all features are numeric
X_train = train_df[features].select_dtypes(include=[np.number])
y_train = train_df[target]

# Handle missing and infinite values in training data
X_train.replace([np.inf, -np.inf], np.nan, inplace=True)
X_train.fillna(X_train.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)

# ------------------------------
# 6. Model Training
# ------------------------------

# Initialize TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits=5)

# Define parameter grid for GridSearchCV
param_grid = {
    'n_estimators': [100],
    'max_depth': [10],
    'min_samples_split': [2],
    'min_samples_leaf': [1]
}

# Initialize GridSearchCV with RandomForestRegressor
grid_search = GridSearchCV(
    estimator=RandomForestRegressor(random_state=42, n_jobs=-1),
    param_grid=param_grid,
    cv=tscv,
    scoring='neg_mean_squared_error',
    verbose=2,
    n_jobs=-1
)

# Fit the model
grid_search.fit(X_train, y_train)

# Print best parameters
print("Best Parameters:", grid_search.best_params_)

# ------------------------------
# 7. Model Evaluation
# ------------------------------

# Cross-validated RMSE scores
from sklearn.model_selection import cross_val_score

cv_scores = cross_val_score(
    grid_search.best_estimator_,
    X_train,
    y_train,
    cv=tscv,
    scoring='neg_mean_squared_error',
    n_jobs=-1
)

rmse_scores = np.sqrt(-cv_scores)
print("Cross-Validated RMSE Scores:", rmse_scores)
print("Average RMSE:", rmse_scores.mean())

# Feature Importances
importances = grid_search.best_estimator_.feature_importances_
indices = np.argsort(importances)[::-1]
feature_names = X_train.columns

plt.figure(figsize=(12,6))
plt.title('Feature Importances')
plt.bar(range(len(importances)), importances[indices], align='center')
plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)
plt.tight_layout()
plt.show()

# ------------------------------
# 8. Prepare Test Data for Prediction
# ------------------------------

# Apply the same feature engineering steps to test_df

# Create lag features in test data using last known 'Power' values from train_df
last_power_values = train_df['Power'].iloc[-max(lags):].values
for lag in lags:
    test_df[f'Power_lag_{lag}'] = last_power_values[-lag]

# Create rolling statistics in test data using last known values from train_df
for window in window_sizes:
    rolling_mean = train_df[f'RollingMean_{window}'].iloc[-1]
    rolling_std = train_df[f'RollingStd_{window}'].iloc[-1]
    test_df[f'RollingMean_{window}'] = rolling_mean
    test_df[f'RollingStd_{window}'] = rolling_std

# Create interaction feature in test data
if 'WS_10m' in test_df.columns:
    last_power = train_df['Power'].iloc[-1]
    test_df['WindSpeed_Power'] = test_df['WS_10m'] * last_power
else:
    print("'WS_10m' not found in test_df.")

# Ensure features are consistent between train and test data
features = [feature for feature in features if feature in test_df.columns]

# Prepare X_test
X_test = test_df[features].select_dtypes(include=[np.number])

# Handle missing and infinite values in test data
X_test.replace([np.inf, -np.inf], np.nan, inplace=True)
X_test.fillna(method='ffill', inplace=True)
X_test.fillna(X_test.mean(), inplace=True)

# ------------------------------
# 9. Make Predictions on Test Data
# ------------------------------

# Make predictions
try:
    test_df['Predicted_Power'] = grid_search.best_estimator_.predict(X_test)
    print("Predictions added to test_df.")
except Exception as e:
    print("Error during prediction:", e)

# Verify that 'Predicted_Power' is in test_df
if 'Predicted_Power' in test_df.columns:
    print("'Predicted_Power' successfully added to test_df.")
else:
    print("'Predicted_Power' not found in test_df.")

# ------------------------------
# 10. Prepare Submission File
# ------------------------------

# Prepare the submission DataFrame
submission_df = test_df[['Timestamp', 'Predicted_Power']].copy()
submission_df.rename(columns={'Timestamp': 'Timestamp', 'Predicted_Power': 'Power'}, inplace=True)

# Save to CSV
submission_df.to_csv('Submission.csv', index=False)
print("Submission file 'Submission.csv' has been created.")

# ------------------------------
# 11. Conclusion
# ------------------------------

print("Model training and prediction completed successfully.")

submission_df.to_csv('Submission.csv', index=False)
print("Submission file 'Submission.csv' has been created.")

import pandas as pd

# Specify the path to your CSV file
file_path = 'Submission.csv'

# Read the CSV file into a DataFrame
df = pd.read_csv(file_path)

# Display the first few rows of the DataFrame
print("First few rows of the CSV file:")
print(df.head())

# Display information about the DataFrame (optional)
print("\nInformation about the CSV file:")
print(df.info())

# Display summary statistics (optional)
print("\nSummary statistics of the CSV file:")
print(df.describe())

